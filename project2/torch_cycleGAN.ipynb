{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-23T16:59:58.156289Z","iopub.status.busy":"2022-04-23T16:59:58.156039Z","iopub.status.idle":"2022-04-23T16:59:59.255732Z","shell.execute_reply":"2022-04-23T16:59:59.255040Z","shell.execute_reply.started":"2022-04-23T16:59:58.156260Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    print(dirname, len(filenames))\n","    \n","#    for filename in filenames:\n","#        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-04-23T16:59:59.262950Z","iopub.status.busy":"2022-04-23T16:59:59.259775Z","iopub.status.idle":"2022-04-23T17:00:01.049708Z","shell.execute_reply":"2022-04-23T17:00:01.048968Z","shell.execute_reply.started":"2022-04-23T16:59:59.262908Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import torchvision.transforms as transforms\n","from torchvision.utils import make_grid\n","\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import shutil\n","\n","print(torch.__version__)\n","if torch.cuda.is_available():\n","    device = 'cuda:0'\n","else:\n","    device = 'cpu'\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["Residual block and generator"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-04-23T17:00:01.052592Z","iopub.status.busy":"2022-04-23T17:00:01.052390Z","iopub.status.idle":"2022-04-23T17:00:01.069906Z","shell.execute_reply":"2022-04-23T17:00:01.068736Z","shell.execute_reply.started":"2022-04-23T17:00:01.052567Z"},"trusted":true},"outputs":[],"source":["class ResBlock(nn.Module):\n","    def __init__(self, in_channels):\n","        super(ResBlock, self).__init__()\n","        self.reflect = nn.ReflectionPad2d(1)\n","        self.conv1 = nn.Conv2d(in_channels, in_channels, 3)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.norm = nn.InstanceNorm2d(in_channels)\n","        \n","    def forward(self, x):\n","        origin_x = x\n","        x = self.reflect(x)\n","        x = self.conv1(x)\n","        x = self.relu(x)\n","        x = self.reflect(x)\n","        x = self.conv1(x)\n","        x = self.norm(x)\n","        return origin_x + x\n","\n","class Generator(nn.Module):\n","    def __init__(self, img_channels=3, out_channels=64, residual_num=9):\n","        super(Generator, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.ReflectionPad2d(img_channels),\n","            nn.Conv2d(img_channels, out_channels, kernel_size=2*img_channels+1, stride=1),\n","            nn.InstanceNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","        self.down_sample = nn.Sequential(nn.Conv2d(out_channels, out_channels*2, kernel_size=img_channels, stride=2, padding=1),\n","                                  nn.InstanceNorm2d(out_channels*2),\n","                                  nn.ReLU(inplace=True),\n","                                  nn.Conv2d(out_channels*2, out_channels*4, kernel_size=img_channels, stride=2, padding=1),\n","                                  nn.InstanceNorm2d(out_channels*4),\n","                                  nn.ReLU(inplace=True),\n","        )\n","        \n","        self.resnet = [ResBlock(out_channels*4) for _ in range(residual_num)]\n","        self.resnet = nn.Sequential(*self.resnet)\n","\n","        self.up_sample = nn.Sequential(nn.Upsample(scale_factor=2), \n","                                nn.Conv2d(4*out_channels, 2*out_channels, kernel_size=img_channels, stride=1, padding=1),\n","                                nn.InstanceNorm2d(out_channels*2),\n","                                nn.ReLU(inplace=True),\n","                                nn.Upsample(scale_factor=2),\n","                                nn.Conv2d(2*out_channels, out_channels, kernel_size=img_channels, stride=1, padding=1),\n","                                nn.InstanceNorm2d(out_channels),\n","                                nn.ReLU(inplace=True),\n","        )\n","        self.out_put = nn.Sequential(\n","            nn.ReflectionPad2d(img_channels),\n","            nn.Conv2d(out_channels, img_channels, kernel_size=2*img_channels+1),\n","            nn.Tanh()\n","        )\n","    \n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.down_sample(x)\n","        x = self.resnet(x)\n","        x = self.up_sample(x)\n","        return self.out_put(x)"]},{"cell_type":"markdown","metadata":{},"source":["Discriminator"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-04-23T17:00:01.071477Z","iopub.status.busy":"2022-04-23T17:00:01.071173Z","iopub.status.idle":"2022-04-23T17:00:01.083579Z","shell.execute_reply":"2022-04-23T17:00:01.082891Z","shell.execute_reply.started":"2022-04-23T17:00:01.071430Z"},"trusted":true},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, img_channels=3, out_channels=64):\n","        super(Discriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(img_channels, out_channels, kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            \n","            nn.Conv2d(out_channels, out_channels*2, kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n","            nn.InstanceNorm2d(out_channels*2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            \n","            nn.Conv2d(out_channels*2, out_channels*4, kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n","            nn.InstanceNorm2d(out_channels*4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            \n","            nn.Conv2d(out_channels*4, out_channels*8, kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n","            nn.InstanceNorm2d(out_channels*8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            \n","            nn.ReflectionPad2d((1,0,1,0)),\n","            nn.Conv2d(out_channels*8, 1, kernel_size=4, padding=1)\n","        )\n","        \n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-04-23T17:00:01.086643Z","iopub.status.busy":"2022-04-23T17:00:01.086262Z","iopub.status.idle":"2022-04-23T17:00:03.975496Z","shell.execute_reply":"2022-04-23T17:00:03.974767Z","shell.execute_reply.started":"2022-04-23T17:00:01.086609Z"},"trusted":true},"outputs":[],"source":["gen_m2p = Generator().to(device)\n","dis_m = Discriminator().to(device)\n","gen_p2m = Generator().to(device)\n","dis_p = Discriminator().to(device)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-04-23T17:00:03.977249Z","iopub.status.busy":"2022-04-23T17:00:03.976985Z","iopub.status.idle":"2022-04-23T17:00:03.989704Z","shell.execute_reply":"2022-04-23T17:00:03.988862Z","shell.execute_reply.started":"2022-04-23T17:00:03.977213Z"},"trusted":true},"outputs":[],"source":["learning_rate = 0.0002\n","betas_range = tuple([0.5, 0.999])\n","\n","gen_m2p_optim = optim.Adam(gen_m2p.parameters(), lr=learning_rate, betas=betas_range)\n","gen_p2m_optim = optim.Adam(gen_p2m.parameters(), lr=learning_rate, betas=betas_range)\n","dis_m_optim = optim.Adam(dis_m.parameters(), lr=learning_rate, betas=betas_range)\n","dis_p_optim = optim.Adam(dis_p.parameters(), lr=learning_rate, betas=betas_range)\n","\n","gan_loss = nn.MSELoss().to(device)\n","cycle_loss = nn.L1Loss().to(device)\n","id_loss = nn.L1Loss().to(device)\n","\n","epoch_num = 50\n","start_epoch = epoch_num // 5\n","lambda_val = lambda epoch: 1 - max(0, epoch-start_epoch) / (epoch_num-start_epoch)  \n","gen_m2p_lr = optim.lr_scheduler.LambdaLR(gen_m2p_optim, lr_lambda=lambda_val)\n","gen_p2m_lr = optim.lr_scheduler.LambdaLR(gen_p2m_optim, lr_lambda=lambda_val)\n","dis_m_lr = optim.lr_scheduler.LambdaLR(dis_m_optim, lr_lambda=lambda_val)\n","dis_p_lr = optim.lr_scheduler.LambdaLR(dis_p_optim, lr_lambda=lambda_val)"]},{"cell_type":"markdown","metadata":{},"source":["Process the dataset"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-04-23T17:00:03.991657Z","iopub.status.busy":"2022-04-23T17:00:03.991385Z","iopub.status.idle":"2022-04-23T17:00:04.016646Z","shell.execute_reply":"2022-04-23T17:00:04.015927Z","shell.execute_reply.started":"2022-04-23T17:00:03.991621Z"},"trusted":true},"outputs":[],"source":["class MonetDataset(Dataset):\n","    def __init__(self, data_path, mode=0, transformer=None):\n","        monet_path = os.path.join(data_path, 'monet_jpg')\n","        photo_path = os.path.join(data_path, 'photo_jpg')\n","        monet_list = os.listdir(monet_path)\n","        photo_list = os.listdir(photo_path)\n","        self.length = min(len(monet_list), len(photo_list))\n","        self.transformer = transformer\n","        div = 250\n","        down_bound, up_bound = div * mode, div * (1 - mode) + self.length * mode\n","        self.monet_images = [os.path.join(monet_path, _) for _ in monet_list[down_bound:up_bound]]\n","        self.photo_images = [os.path.join(photo_path, _) for _ in photo_list[down_bound:up_bound]]\n","        \n","    def __len__(self):\n","        return self.length\n","    \n","    def __getitem__(self, idx):\n","        monet_exp = self.monet_images[idx % len(self.monet_images)]\n","        photo_exp = self.photo_images[idx % len(self.photo_images)]\n","        monet_exp = Image.open(monet_exp)\n","        photo_exp = Image.open(photo_exp)\n","        \n","        if self.transformer:\n","            monet_exp = self.transformer(monet_exp)\n","            photo_exp = self.transformer(photo_exp)\n","        return monet_exp, photo_exp\n","\n","data_location = '/kaggle/input/gan-getting-started'\n","img_size = 256\n","transformer_train = transforms.Compose([\n","                transforms.Resize((img_size, img_size)),\n","                transforms.RandomHorizontalFlip(),\n","                transforms.RandomVerticalFlip(),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.5], std=[0.5])\n","            ])\n","\n","transformer_test = transforms.Compose([\n","                transforms.Resize((img_size, img_size)),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.5], std=[0.5])\n","            ])\n","\n","batch_size = 5\n","worker_num = 2\n","# mode=0 represent train set, mode=1 represent test set\n","trainloader = DataLoader(\n","    MonetDataset(data_location, mode=0, transformer=transformer_train),\n","    batch_size = batch_size,\n","    shuffle = True,\n","    num_workers = worker_num\n",")\n","\n","testloader = DataLoader(\n","    MonetDataset(data_location, mode=1, transformer=transformer_test),\n","    batch_size = batch_size,\n","    shuffle = False,\n","    num_workers = worker_num\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Show some of the images"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-04-23T17:00:04.019572Z","iopub.status.busy":"2022-04-23T17:00:04.019384Z","iopub.status.idle":"2022-04-23T17:00:10.487434Z","shell.execute_reply":"2022-04-23T17:00:10.486733Z","shell.execute_reply.started":"2022-04-23T17:00:04.019550Z"},"trusted":true},"outputs":[],"source":["def show_samples(images):\n","    monet_real, photo_real = images[0].to(device), images[1].to(device)\n","    image_num = min(monet_real.size(0), 5)\n","    gen_m2p.eval()\n","    gen_p2m.eval()\n","    monet_fake = gen_m2p(monet_real).detach()\n","    photo_fake = gen_p2m(photo_real).detach()\n","    figure_size = tuple([4*image_num, 4*image_num])\n","    \n","    var_name = [\"monet_real\", \"monet_fake\", \"photo_real\", \"photo_fake\"]\n","    title_name = [\"real monet image\", \"m2p image\", \"real photo image\", \"p2m image\"]\n","    for i in range(len(var_name)):\n","        cur_image = make_grid(eval(var_name[i]), nrow=image_num, normalize=True)\n","        plt.figure(figsize=figure_size)\n","        plt.imshow(cur_image.cpu().permute(1, 2, 0))\n","        plt.axis('off')\n","        plt.title(title_name[i])\n","        plt.show()\n","\n","image_to_show = next(iter(testloader))\n","show_samples(image_to_show)\n","\n","def show_samples1(images):\n","    image_num = min(images.size(0), 5)\n","    gen_m2p.eval()\n","    gen_p2m.eval()\n","    monet_fake = gen_m2p(images).detach()\n","    \n","    figure_size = tuple([4*image_num, 4*image_num])\n","    \n","    var_name = [\"images\", \"monet_fake\"]\n","    for i in range(len(var_name)):\n","        cur_image = make_grid(eval(var_name[i]), nrow=image_num, normalize=True)\n","        plt.figure(figsize=figure_size)\n","        plt.imshow(cur_image.cpu().permute(1, 2, 0))\n","        plt.axis('off')\n","        plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Train the model"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-04-23T17:00:10.489520Z","iopub.status.busy":"2022-04-23T17:00:10.489095Z","iopub.status.idle":"2022-04-23T17:01:14.510284Z","shell.execute_reply":"2022-04-23T17:01:14.508585Z","shell.execute_reply.started":"2022-04-23T17:00:10.489481Z"},"trusted":true},"outputs":[],"source":["model_list = [\"gen_m2p\", \"gen_p2m\", \"dis_m\", \"dis_p\"]\n","scale_size = 16\n","real_img = torch.ones([batch_size, 1, img_size // scale_size, img_size // scale_size]).to(device)\n","fake_img = torch.zeros([batch_size, 1, img_size // scale_size, img_size // scale_size]).to(device)\n","loss_val = 5\n","for epoch in range(epoch_num):\n","    for train_data in trainloader:\n","        monet_real, photo_real = train_data[0].to(device), train_data[1].to(device)\n","        # Train Generators G and F\n","        for i in range(2):\n","            eval(model_list[i]).train()\n","            eval(model_list[i] + \"_optim\").zero_grad()\n","        photo_fake = gen_m2p(monet_real)\n","        monet_fake = gen_p2m(photo_real)\n","        \n","        # Loss_idt and Loss_GAN\n","        loss_idt = (id_loss(photo_fake, monet_real) + id_loss(monet_fake, photo_real)) * 0.5\n","        loss_GAN = (gan_loss(dis_p(photo_fake), real_img) + gan_loss(dis_m(monet_fake), real_img)) * 0.5\n","        \n","        # Loss_Cycle and Loss_Total\n","        m2p2m = gen_p2m(photo_fake)\n","        p2m2p = gen_m2p(monet_fake)\n","        loss_cycle = cycle_loss(m2p2m, monet_real) + cycle_loss(p2m2p, photo_real)\n","        loss_G = loss_val * (loss_idt + loss_cycle) + loss_GAN\n","        loss_G.backward()\n","        \n","        # Train Discriminator P and Train Discriminator M\n","        for i in range(2):\n","            eval(model_list[i+2] + \"_optim\").zero_grad()\n","        loss_D_P = (gan_loss(dis_m(monet_real), real_img) + gan_loss(dis_m(monet_fake.detach()), fake_img)) * 0.5\n","        loss_D_P.backward()\n","        loss_D_M = (gan_loss(dis_p(photo_real), real_img) + gan_loss(dis_p(photo_fake.detach()), fake_img)) * 0.5\n","        loss_D_M.backward()\n","        \n","        # step the optimizers\n","        for i in range(len(model_list)):\n","            eval(model_list[i]+'_optim').step()\n","        \n","    for i in range(len(model_list)):\n","        eval(model_list[i]+\"_lr\").step()\n","    \n","    print(\"epoch\" + str(epoch + 1) + \" is finished\")\n","    loss_D = (loss_D_P + loss_D_M) * 0.5\n","    print(\"Current epoch is: \" + str(epoch + 1) + ' of ' + str(epoch_num))\n","    print(\"Generators loss:\",loss_G.item(), \"Loss_idt:\" , loss_idt.item())\n","    print(\"Loss_GAN:\", loss_GAN.item(), \"Loss_Cycle:\", loss_cycle.item())\n","    print(\"Discriminators loss:\", loss_D.item(), \"D_P:\", loss_D_P.item(), \"D_M:\", loss_D_M.item())   \n","    if (epoch + 1) % 10 == 0:\n","        show_samples(image_to_show) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-23T17:01:18.109474Z","iopub.status.busy":"2022-04-23T17:01:18.109178Z"},"trusted":true},"outputs":[],"source":["save_dir = '../images'\n","if not os.path.exists(save_dir):\n","    os.makedirs(save_dir)\n","# This submission part refers to https://www.kaggle.com/code/lmyybh/pytorch-cyclegan \n","# For other parts, a very few thoughts are also learnt from it, they are too sparse to be cited.\n","# We wrote all the codes basing on our own understanding for our own design.\n","photo_path = os.path.join(data_location, 'photo_jpg')\n","files = [os.path.join(photo_path, name) for name in os.listdir(photo_path)]\n","\n","norm_val = tuple([0.5] * 3)\n","generate_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(norm_val, norm_val)\n","])\n","\n","to_image = transforms.ToPILImage()\n","gen_p2m.eval()\n","for i in range(0, len(files), batch_size):\n","    imgs = []\n","    limit = min(len(files), i + batch_size)\n","    for j in range(i, limit):\n","        img = Image.open(files[j])\n","        imgs.append(generate_transforms(img))\n","    imgs = torch.stack(imgs, 0).to(device)\n","    fake_imgs = gen_p2m(imgs).detach().cpu()\n","    image_num = fake_imgs.size(0)\n","    for j in range(image_num):\n","        img = fake_imgs[j].squeeze().permute(1, 2, 0)\n","        img_arr = img.numpy()\n","        img_arr = (img_arr - np.min(img_arr)) * 255 / (np.max(img_arr) - np.min(img_arr))\n","        img = to_image(img_arr.astype(np.uint8))\n","        _, name = os.path.split(files[i+j])\n","        img.save(os.path.join(save_dir, name))"]},{"cell_type":"markdown","metadata":{},"source":["compress and submit"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-04-23T17:01:14.512877Z","iopub.status.idle":"2022-04-23T17:01:14.513223Z","shell.execute_reply":"2022-04-23T17:01:14.513093Z","shell.execute_reply.started":"2022-04-23T17:01:14.513073Z"},"trusted":true},"outputs":[],"source":["shutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")\n","torch.save(gen_m2p.state_dict(),'./gen_m2p.pt')\n","torch.save(dis_m.state_dict(),'./dis_m.pt')\n","torch.save(gen_p2m.state_dict(),'./gen_p2m.pt')\n","torch.save(dis_p.state_dict(),'./dis_p.pt')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
